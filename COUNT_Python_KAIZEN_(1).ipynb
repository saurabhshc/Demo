{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db0fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out_path = r'\\\\apanzp-ainas001\\Dept\\ACTGRP\\Infor\\SysRef\\Data Mart\\Source_Reconciliations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abe1f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source Server and Database (main source where data is actualy fetched from)\n",
    "\n",
    "Source_Server = 'APAUKZNUDB191.aceins.com'\n",
    "Source_Database = 'KaizenProductionBackup'\n",
    "Source_Schema = 'dbo'\n",
    "\n",
    "#Destination Server and Database  (our destination database where using ETL / SSIS  data is saved at)\n",
    "\n",
    "Destination_Server = 'APAUACTUAP161'\n",
    "Destination_Database = 'DW_Actuarial'\n",
    "Destination_Schema = 'KAIZEN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6121ba81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\ntable_list_DW = ['Authority', 'Branch', 'Broker', 'Client', 'CODEX_Booking', 'CODEX_CCI', 'CODEX_Policy_Other', 'CODEX_Stamps', \\n                  'CODEX_UWInfo', 'KTS_Registry', 'Nexus2_Main', 'Policy_Basic', 'Product', 'Users', \\n                  'Quokka_PARTONE', 'Quokka_PARTTHREE', 'Quokka_PARTTWO']\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The table names defined in a LIST variable \"table_list_src\" and \"table_list_dest\"\n",
    "\n",
    "table_list_SRC = ['Authority', 'Branch', 'Broker', 'Client', 'CODEX_Booking', 'CODEX_CCI', 'CODEX_Policy_Other', 'CODEX_Stamps', \n",
    "                  'CODEX_UWInfo', 'KTS_Registry', 'Nexus2_Main', 'Policy_Basic', 'Product', 'Users', \n",
    "                  'Quokka_PARTONE', 'Quokka_PARTTHREE', 'Quokka_PARTTWO']\n",
    "\n",
    "table_list_DW = ['Authority', 'Branch', 'Broker', 'Client', 'CODEXBooking', 'CODEXCCI', 'CODEXPolicyOther', 'CODEXStamps', \n",
    "                   'CODEXUWInfo', 'KTSRegistry', 'Nexus2Main', 'PolicyBasic', 'Product', 'Users',\n",
    "                 'Quokka_PARTONE', 'Quokka_PARTTHREE', 'Quokka_PARTTWO']\n",
    "'''\n",
    "\n",
    "table_list_DW = ['Authority', 'Branch', 'Broker', 'Client', 'CODEX_Booking', 'CODEX_CCI', 'CODEX_Policy_Other', 'CODEX_Stamps', \n",
    "                  'CODEX_UWInfo', 'KTS_Registry', 'Nexus2_Main', 'Policy_Basic', 'Product', 'Users', \n",
    "                  'Quokka_PARTONE', 'Quokka_PARTTHREE', 'Quokka_PARTTWO']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1b637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38cfb312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'plyer' package already exists in your system\n",
      "'pyodbc' package already exists in your system\n",
      "'pandas' package already exists in your system\n",
      "'sqlalchemy' package already exists in your system\n",
      "'sqldf' package already exists in your system\n",
      "'pandasql' package already exists in your system\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "\n",
    "# Install packages if they are not already installed\n",
    "def install_package(package_name):\n",
    "    try:\n",
    "        importlib.import_module(package_name)\n",
    "        print(f\"'{package_name}' package already exists in your system\")\n",
    "    except ImportError:\n",
    "        subprocess.call(['pip', 'install', '--trusted-host', 'pypi.org', '--trusted-host', 'pypi.python.org', '--trusted-host', 'files.pythonhosted.org', package_name, '-q'])\n",
    "        print(f\"'{package_name}' package is now installed\")\n",
    "\n",
    "# Install required packages\n",
    "install_package('plyer')\n",
    "install_package('pyodbc')\n",
    "install_package('pandas')\n",
    "install_package('sqlalchemy')\n",
    "install_package('sqldf')\n",
    "install_package('pandasql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6c60b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from plyer import notification\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import sqlalchemy as db\n",
    "import os\n",
    "from pandasql import sqldf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pyodbc.drivers()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import datetime\n",
    "current_date = datetime.datetime.now().strftime('%d_%m_%Y')\n",
    "\n",
    "st_fl_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c830ecc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d17f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create SQLAlchemy engine with a connection string - DW_Actuarial - DESTINATION\n",
    "conn_str = (\n",
    "        \"Driver={SQL Server Native Client 11.0};\"\n",
    "        f\"Server={Source_Server};\"\n",
    "        f\"Database={Source_Database};\"\n",
    "        \"Trusted_Connection=yes;\"\n",
    "    )\n",
    "\n",
    "engine_SRC = db.create_engine(f'mssql+pyodbc:///?odbc_connect={conn_str}')\n",
    "\n",
    "\n",
    "\n",
    "conn_str = (\n",
    "        \"Driver={SQL Server Native Client 11.0};\"\n",
    "        f\"Server={Destination_Server};\"\n",
    "        f\"Database={Destination_Database};\"\n",
    "        \"Trusted_Connection=yes;\"\n",
    "    )\n",
    "\n",
    "engine_DW = db.create_engine(f'mssql+pyodbc:///?odbc_connect={conn_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e1cdce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f23823a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KAIZEN_SRC(table_list_SRC):\n",
    "    # Define queries and create a dictionary with keys as \"query{i}\"\n",
    "    global engine_SRC\n",
    "    num_names = len(table_list_SRC)\n",
    "    key_names = [f\"query{i}\" for i in range(1, num_names+1)]\n",
    "    values_names = []\n",
    "    for tbl_name in table_list_SRC:\n",
    "        \n",
    "        query_SRC = f\"\"\"SELECT '{tbl_name}' AS TABLE_NAME_SRC,\n",
    "                   COUNT(*) AS SQL_SERVER_ROW_COUNT_SRC,       \n",
    "                   (SELECT COUNT(*) FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = '{Source_Schema}' AND TABLE_NAME = '{tbl_name}' \n",
    "                   AND COLUMN_NAME NOT IN ('CopyDate', 'ParentExecutionID')) AS SQL_SERVER_COL_COUNT_SRC,\n",
    "                   STUFF((SELECT ', ' + COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS \n",
    "                      WHERE TABLE_SCHEMA = '{Source_Schema}' AND TABLE_NAME = '{tbl_name}' AND COLUMN_NAME NOT IN ('CopyDate', 'ParentExecutionID')\n",
    "                      FOR XML PATH('')), 1, 2, '') AS COLUMN_NAMES_SRC,\n",
    "                   CONVERT(datetime, SWITCHOFFSET(SYSDATETIMEOFFSET(), '+10:30')) AS CEN_AUSTRALIA_STANDARD_TIME,\n",
    "                   CONVERT(datetime, SWITCHOFFSET(SYSDATETIMEOFFSET(), '+05:30')) AS INDIAN_STANDARD_TIME\n",
    "                   FROM {Source_Database}.{Source_Schema}.{tbl_name}\"\"\"\n",
    "           \n",
    "        values_names.append(query_SRC)\n",
    "    queries_SRC = dict(zip(key_names, values_names))\n",
    "    \n",
    "    \n",
    "    # Loop through the queries and execute them\n",
    "    result_dict = {}\n",
    "    for query_name, query_SRC in queries_SRC.items():\n",
    "        # Execute query and create a Pandas DataFrame\n",
    "        df = pd.read_sql(query_SRC, engine_SRC)\n",
    "        result_dict[query_name] = df\n",
    "\n",
    "    # Combine all query results into a single pandas DataFrame\n",
    "    combined_df_SRC = pd.concat([result_dict[q] for q in result_dict])\n",
    "\n",
    "    # Add a new column \"query_name\" to indicate which query each row belongs to\n",
    "    combined_df_SRC = combined_df_SRC.assign(query_name=[q for q in result_dict for _ in range(len(result_dict[q]))])\n",
    "\n",
    "    # Order the DataFrame by the \"Table_Name\" column\n",
    "    combined_df_SRC = combined_df_SRC.sort_values(by=['TABLE_NAME_SRC'])\n",
    "\n",
    "    # Return the Pandas DataFrame\n",
    "    return combined_df_SRC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12203a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36afbdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KAIZEN_SRC_COL_DETAILS(table_list_SRC):\n",
    "    # Define queries and create a dictionary with keys as \"query{i}\"\n",
    "    global engine_SRC\n",
    "    num_names = len(table_list_SRC)\n",
    "    key_names = [f\"query{i}\" for i in range(1, num_names+1)]\n",
    "    values_names = []\n",
    "    for tbl_name in table_list_SRC:\n",
    "        \n",
    "        query_SRC = f\"\"\"SELECT TABLE_NAME AS TABLE_NAME_SRC, \n",
    "                         TABLE_SCHEMA AS TABLE_SCHEMA_SRC, \n",
    "                         COLUMN_NAME AS COLUMN_NAME_SRC,\n",
    "                         CASE WHEN DATA_TYPE IN ('decimal', 'numeric') \n",
    "                              THEN CONCAT(DATA_TYPE, ' (', CHARACTER_MAXIMUM_LENGTH, NUMERIC_PRECISION, ',', NUMERIC_SCALE, ')')\n",
    "                              WHEN DATA_TYPE IN ('varchar', 'char', 'nvarchar')\n",
    "                              THEN CONCAT(DATA_TYPE, ' (', CHARACTER_MAXIMUM_LENGTH, ')')\n",
    "                              WHEN DATA_TYPE IN ('int', 'bigint', 'datetime') THEN DATA_TYPE\n",
    "                              ELSE DATA_TYPE\n",
    "                              END AS DATA_TYPE_WITH_LENGTH_SRC,\n",
    "                         CONVERT(datetime, SWITCHOFFSET(SYSDATETIMEOFFSET(), '+10:30')) AS CEN_AUSTRALIA_STANDARD_TIME,\n",
    "                         CONVERT(datetime, SWITCHOFFSET(SYSDATETIMEOFFSET(), '+05:30')) AS INDIAN_STANDARD_TIME    \n",
    "                         FROM INFORMATION_SCHEMA.COLUMNS\n",
    "                         WHERE TABLE_NAME = '{tbl_name}' AND TABLE_SCHEMA = '{Source_Schema}' AND \n",
    "                               COLUMN_NAME NOT IN ('CopyDate', 'ParentExecutionID')\n",
    "                         ORDER BY TABLE_NAME, COLUMN_NAME\"\"\"\n",
    "           \n",
    "        values_names.append(query_SRC)\n",
    "    queries_SRC = dict(zip(key_names, values_names))\n",
    "    \n",
    "    \n",
    "    # Loop through the queries and execute them\n",
    "    result_dict = {}\n",
    "    for query_name, query_SRC in queries_SRC.items():\n",
    "        # Execute query and create a Pandas DataFrame\n",
    "        df = pd.read_sql(query_SRC, engine_SRC)\n",
    "        result_dict[query_name] = df\n",
    "\n",
    "    # Combine all query results into a single pandas DataFrame\n",
    "    combined_df_SRC_COL_DETAILS = pd.concat([result_dict[q] for q in result_dict])\n",
    "\n",
    "    # Add a new column \"query_name\" to indicate which query each row belongs to\n",
    "    combined_df_SRC_COL_DETAILS = combined_df_SRC_COL_DETAILS.assign(query_name=[q for q in result_dict for _ in range(len(result_dict[q]))])\n",
    "\n",
    "    # Order the DataFrame by the \"Table_Name\" column\n",
    "    combined_df_SRC_COL_DETAILS = combined_df_SRC_COL_DETAILS.sort_values(by=['TABLE_NAME_SRC', 'COLUMN_NAME_SRC'])\n",
    "\n",
    "    # Return the Pandas DataFrame\n",
    "    return combined_df_SRC_COL_DETAILS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd9a3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b4e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KAIZEN_DW(table_list_DW):\n",
    "    # Define queries and create a dictionary with keys as \"query{i}\"\n",
    "    global engine_DW\n",
    "    num_names = len(table_list_DW)\n",
    "    key_names = [f\"query{i}\" for i in range(1, num_names+1)]\n",
    "    values_names = []\n",
    "    for tbl_name in table_list_DW:\n",
    "        \n",
    "        query_DW = f\"\"\"SELECT '{tbl_name}' AS TABLE_NAME_DW,\n",
    "                   COUNT(*) AS SQL_SERVER_ROW_COUNT_DW,       \n",
    "                   (SELECT COUNT(*) FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = '{Destination_Schema}' AND TABLE_NAME = '{tbl_name}' \n",
    "                   AND COLUMN_NAME NOT IN ('CopyDate', 'ParentExecutionID')) AS SQL_SERVER_COL_COUNT_DW,\n",
    "                   STUFF((SELECT ', ' + COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS \n",
    "                      WHERE TABLE_SCHEMA = '{Destination_Schema}' AND TABLE_NAME = '{tbl_name}' AND COLUMN_NAME NOT IN ('CopyDate', 'ParentExecutionID')\n",
    "                      FOR XML PATH('')), 1, 2, '') AS COLUMN_NAMES_DW,\n",
    "                   CONVERT(datetime, SWITCHOFFSET(SYSDATETIMEOFFSET(), '+10:30')) AS CEN_AUSTRALIA_STANDARD_TIME,\n",
    "                   CONVERT(datetime, SWITCHOFFSET(SYSDATETIMEOFFSET(), '+05:30')) AS INDIAN_STANDARD_TIME\n",
    "                   FROM {Destination_Database}.{Destination_Schema}.{tbl_name}\"\"\"\n",
    "        \n",
    "        values_names.append(query_DW)\n",
    "    queries_DW = dict(zip(key_names, values_names))\n",
    "    \n",
    "    \n",
    "    # Loop through the queries and execute them\n",
    "    result_dict = {}\n",
    "    for query_name, query_DW in queries_DW.items():\n",
    "        # Execute query and create a Pandas DataFrame\n",
    "        df = pd.read_sql(query_DW, engine_DW)\n",
    "        result_dict[query_name] = df\n",
    "\n",
    "    # Combine all query results into a single pandas DataFrame\n",
    "    combined_df_DW = pd.concat([result_dict[q] for q in result_dict])\n",
    "\n",
    "    # Add a new column \"query_name\" to indicate which query each row belongs to\n",
    "    combined_df_DW = combined_df_DW.assign(query_name=[q for q in result_dict for _ in range(len(result_dict[q]))])\n",
    "\n",
    "    # Order the DataFrame by the \"Table_Name\" column\n",
    "    combined_df_DW = combined_df_DW.sort_values(by=['TABLE_NAME_DW'])\n",
    "\n",
    "    # Return the Pandas DataFrame\n",
    "    return combined_df_DW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f0dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1279267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KAIZEN_DW_COL_DETAILS(table_list_DW):\n",
    "    # Define queries and create a dictionary with keys as \"query{i}\"\n",
    "    global engine_DW\n",
    "    num_names = len(table_list_DW)\n",
    "    key_names = [f\"query{i}\" for i in range(1, num_names+1)]\n",
    "    values_names = []\n",
    "    for tbl_name in table_list_DW:\n",
    "        \n",
    "        query_DW = f\"\"\"SELECT TABLE_NAME AS TABLE_NAME_DW, \n",
    "                         TABLE_SCHEMA AS TABLE_SCHEMA_DW, \n",
    "                         COLUMN_NAME AS COLUMN_NAME_DW,\n",
    "                         CASE WHEN DATA_TYPE IN ('decimal', 'numeric') \n",
    "                              THEN CONCAT(DATA_TYPE, ' (', CHARACTER_MAXIMUM_LENGTH, NUMERIC_PRECISION, ',', NUMERIC_SCALE, ')')\n",
    "                              WHEN DATA_TYPE IN ('varchar', 'char', 'nvarchar')\n",
    "                              THEN CONCAT(DATA_TYPE, ' (', CHARACTER_MAXIMUM_LENGTH, ')')\n",
    "                              WHEN DATA_TYPE IN ('int', 'bigint', 'datetime') THEN DATA_TYPE\n",
    "                              ELSE DATA_TYPE\n",
    "                              END AS DATA_TYPE_WITH_LENGTH_DW,\n",
    "                         CONVERT(datetime, SWITCHOFFSET(SYSDATETIMEOFFSET(), '+10:30')) AS CEN_AUSTRALIA_STANDARD_TIME,\n",
    "                         CONVERT(datetime, SWITCHOFFSET(SYSDATETIMEOFFSET(), '+05:30')) AS INDIAN_STANDARD_TIME     \n",
    "                         FROM INFORMATION_SCHEMA.COLUMNS\n",
    "                         WHERE TABLE_NAME = '{tbl_name}' AND TABLE_SCHEMA = '{Destination_Schema}' AND \n",
    "                               COLUMN_NAME NOT IN ('CopyDate', 'ParentExecutionID')\n",
    "                         ORDER BY TABLE_NAME, COLUMN_NAME\"\"\"\n",
    "           \n",
    "        values_names.append(query_DW)\n",
    "    queries_DW = dict(zip(key_names, values_names))\n",
    "    \n",
    "    \n",
    "    # Loop through the queries and execute them\n",
    "    result_dict = {}\n",
    "    for query_name, query_DW in queries_DW.items():\n",
    "        # Execute query and create a Pandas DataFrame\n",
    "        df = pd.read_sql(query_DW, engine_DW)\n",
    "        result_dict[query_name] = df\n",
    "\n",
    "    # Combine all query results into a single pandas DataFrame\n",
    "    combined_df_DW_COL_DETAILS = pd.concat([result_dict[q] for q in result_dict])\n",
    "\n",
    "    # Add a new column \"query_name\" to indicate which query each row belongs to\n",
    "    combined_df_DW_COL_DETAILS = combined_df_DW_COL_DETAILS.assign(query_name=[q for q in result_dict for _ in range(len(result_dict[q]))])\n",
    "\n",
    "    # Order the DataFrame by the \"Table_Name\" column\n",
    "    combined_df_DW_COL_DETAILS = combined_df_DW_COL_DETAILS.sort_values(by=['TABLE_NAME_DW', 'COLUMN_NAME_DW'])\n",
    "\n",
    "    # Return the Pandas DataFrame\n",
    "    return combined_df_DW_COL_DETAILS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43462c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "150a3eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the two functions to get table data\n",
    "#combined_df_SRC = KAIZEN_SRC(table_list_SRC)\n",
    "#combined_df_DW = KAIZEN_DW(table_list_DW)\n",
    "\n",
    "\n",
    "#combined_df = combined_df_SRC.merge(combined_df_DW, on='query_name', how='left')\n",
    "\n",
    "combined_df = pd.concat([KAIZEN_SRC(table_list_SRC).set_index('query_name'), KAIZEN_DW(table_list_DW).loc[:, ['query_name','TABLE_NAME_DW', 'SQL_SERVER_ROW_COUNT_DW', 'SQL_SERVER_COL_COUNT_DW', 'COLUMN_NAMES_DW']].set_index('query_name')], axis=1, join='outer')\n",
    "\n",
    "\n",
    "\n",
    "def remove_common_words(string1, string2):\n",
    "    list1 = string1.split(', ')\n",
    "    list2 = string2.split(', ')\n",
    "    common_words = set(list1) & set(list2)\n",
    "    for word in common_words:\n",
    "        list1.remove(word)\n",
    "        list2.remove(word)    \n",
    "    return ', '.join(list1 + list2)\n",
    "\n",
    "\n",
    "# create new column with removed common words\n",
    "combined_df['MISSING_COLUMN_NAMES'] = combined_df.apply(lambda x: remove_common_words(x['COLUMN_NAMES_SRC'], x['COLUMN_NAMES_DW']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea06e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a28159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This query gves us all table details details - one below another\n",
    "from pandasql import sqldf\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT TABLE_NAME_SRC, TABLE_NAME_DW, SQL_SERVER_ROW_COUNT_SRC, SQL_SERVER_ROW_COUNT_DW, \n",
    "            (SQL_SERVER_ROW_COUNT_DW - SQL_SERVER_ROW_COUNT_SRC) AS `ROW_DIFFERENCE (SQLSERVERDW - SQLSERVERSRC)`,           \n",
    "            SQL_SERVER_COL_COUNT_SRC, SQL_SERVER_COL_COUNT_DW, \n",
    "            (SQL_SERVER_COL_COUNT_DW - SQL_SERVER_COL_COUNT_SRC) AS `COL_DIFFERENCE (SQLSERVERDW - SQLSERVERSRC)`,\n",
    "            CASE WHEN (SQL_SERVER_COL_COUNT_DW - SQL_SERVER_COL_COUNT_SRC) = 0 THEN NULL\n",
    "                 ELSE MISSING_COLUMN_NAMES\n",
    "            END AS MISSING_COLUMN_NAMES,\n",
    "            CEN_AUSTRALIA_STANDARD_TIME,\n",
    "            INDIAN_STANDARD_TIME\n",
    "        FROM combined_df\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "combined_df_KAIZEN = sqldf(query, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a99aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e87fc608",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This query gives us granular column details - one below another\n",
    "\n",
    "combined_df_SRC_COL_DETAILS = KAIZEN_SRC_COL_DETAILS(table_list_SRC)\n",
    "combined_df_DW_COL_DETAILS = KAIZEN_DW_COL_DETAILS(table_list_DW)\n",
    "\n",
    "\n",
    "from pandasql import sqldf\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT SRC.TABLE_NAME_SRC, DW.TABLE_NAME_DW,\n",
    "        SRC.COLUMN_NAME_SRC, DW.COLUMN_NAME_DW,\n",
    "        SRC.DATA_TYPE_WITH_LENGTH_SRC, DW.DATA_TYPE_WITH_LENGTH_DW,\n",
    "        \n",
    "        CASE \n",
    "        WHEN REPLACE(SUBSTR(SRC.DATA_TYPE_WITH_LENGTH_SRC, '[[:digit:]]+'), '\\\\D+', '') \n",
    "             != REPLACE(SUBSTR(DW.DATA_TYPE_WITH_LENGTH_DW, '[[:digit:]]+'), '\\\\D+', '') \n",
    "        THEN 1\n",
    "        WHEN REPLACE(REPLACE(SRC.DATA_TYPE_WITH_LENGTH_SRC, '\\\\d+', ''), '[^a-zA-Z]+', ', ') \n",
    "             != REPLACE(REPLACE(DW.DATA_TYPE_WITH_LENGTH_DW, '\\\\d+', ''), '[^a-zA-Z]+', ', ') \n",
    "        THEN 2\n",
    "        WHEN REPLACE(SUBSTR(SRC.DATA_TYPE_WITH_LENGTH_SRC, '[[:digit:]]+'), '\\\\D+', '') \n",
    "             != REPLACE(SUBSTR(DW.DATA_TYPE_WITH_LENGTH_DW, '[[:digit:]]+'), '\\\\D+', '') \n",
    "             AND REPLACE(REPLACE(SRC.DATA_TYPE_WITH_LENGTH_SRC, '\\\\d+', ''), '[^a-zA-Z]+', ', ') \n",
    "             != REPLACE(REPLACE(DW.DATA_TYPE_WITH_LENGTH_DW, '\\\\d+', ''), '[^a-zA-Z]+', ', ')\n",
    "        THEN 3\n",
    "        ELSE 0\n",
    "    END AS MISSING_DATA_CODED_INFO,\n",
    "        \n",
    "        SRC.CEN_AUSTRALIA_STANDARD_TIME,\n",
    "        SRC.INDIAN_STANDARD_TIME\n",
    "        FROM combined_df_DW_COL_DETAILS DW\n",
    "        LEFT JOIN combined_df_SRC_COL_DETAILS SRC \n",
    "        ON LOWER(REPLACE(SRC.TABLE_NAME_SRC, '_', '')) = LOWER(REPLACE(DW.TABLE_NAME_DW, '_', ''))\n",
    "        AND LOWER(REPLACE(SRC.COLUMN_NAME_SRC, '_', '')) = LOWER(REPLACE(DW.COLUMN_NAME_DW, '_', ''))\n",
    "\"\"\"\n",
    "\n",
    "combined_df_KAIZEN_COL_DETAILS = sqldf(query, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbaf75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57d188d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_wd=os.getcwd()\n",
    "path=r'\\\\apanzp-ainas001\\Dept\\ACTGRP\\Infor\\SysRef\\Data Mart\\Source_Reconciliations'\n",
    "script_name='COUNT_Python_KAIZEN.ipynb'\n",
    "def writeCoverSheet( writer):\n",
    "\n",
    "    workbook3  = writer.book\n",
    "    worksheet3 = workbook3.add_worksheet('Info')\n",
    "    worksheet3.set_column(1,3,60)\n",
    "    yellow_sum_col = workbook3.add_format({'bold': True,'align':'center','border': 1}) # white cell font color\n",
    "    yellow_sum_col.set_font_name('Times New Roman')\n",
    "    yellow_sum_col.set_font_size(10)\n",
    "    yellow_sum_col.set_align('center')\n",
    "    prod=\"Production\"\n",
    "    if Destination_Server=='APAUACTUAP161':\n",
    "        prod=\"UAT\"\n",
    "      \n",
    "    worksheet3.write(1,1,'Report Generated Date ',yellow_sum_col)\n",
    "    worksheet3.write(1,2,current_date,yellow_sum_col)\n",
    "    worksheet3.write(2,1,'Source Server',yellow_sum_col)\n",
    "    worksheet3.write(2,2,Source_Server,yellow_sum_col)\n",
    "    worksheet3.write(3,1,'Source Database',yellow_sum_col)\n",
    "    worksheet3.write(3,2,Source_Database,yellow_sum_col)\n",
    "    worksheet3.write(4,1,'Source Schema',yellow_sum_col)\n",
    "    worksheet3.write(4,2,Source_Schema,yellow_sum_col)\n",
    "    worksheet3.write(5,1,'Destination Server',yellow_sum_col)\n",
    "    worksheet3.write(5,2,Destination_Server,yellow_sum_col)\n",
    "    worksheet3.write(6,1,'Destination Database',yellow_sum_col)\n",
    "    worksheet3.write(6,2,Destination_Database,yellow_sum_col)\n",
    "    worksheet3.write(7,1,'Destination Schema',yellow_sum_col)\n",
    "    worksheet3.write(7,2,Destination_Schema,yellow_sum_col)\n",
    "\n",
    "    worksheet3.write(8,1,'Environment',yellow_sum_col)\n",
    "    worksheet3.write(8,2,prod,yellow_sum_col)\n",
    "    worksheet3.write(9,1,'Script Path',yellow_sum_col)\n",
    "    worksheet3.write(9,2,path,yellow_sum_col)\n",
    "    worksheet3.write(10,1,'Script Name',yellow_sum_col)\n",
    "    worksheet3.write(10,2,script_name,yellow_sum_col)\n",
    "    worksheet3.write(11,1,'Working Directory of script',yellow_sum_col)\n",
    "    worksheet3.write(11,2,path_wd,yellow_sum_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92d94a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import xlsxwriter\n",
    "\n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "    writer = pd.ExcelWriter(new_out_path + '\\\\' + f\"Reconciliation_{Destination_Schema}_{current_date}.xlsx\", engine='xlsxwriter')\n",
    "\n",
    "    combined_df_KAIZEN.iloc[:,0:10].style.set_properties(**{'text-align': 'center'}).to_excel(writer, sheet_name= f'{Destination_Schema}_COUNTS', startrow=0, startcol=0, header=True, index=False)\n",
    "    combined_df_KAIZEN.iloc[:,5:].style.set_properties(**{'text-align': 'center'}).to_excel(writer, sheet_name= f'{Destination_Schema}_COUNTS', startrow=0, startcol=5, header=True, index=False)\n",
    "    combined_df_KAIZEN_COL_DETAILS.iloc[:,0:4].style.set_properties(**{'text-align': 'center'}).to_excel(writer, sheet_name=f'{Destination_Schema}_COL_DETAILS', startrow=0, startcol=0, header=True, index=False)\n",
    "    combined_df_KAIZEN_COL_DETAILS.iloc[:,4:6].style.set_properties(**{'text-align': 'center'}).to_excel(writer, sheet_name=f'{Destination_Schema}_COL_DETAILS', startrow=0, startcol=5, header=True, index=False)\n",
    "    combined_df_KAIZEN_COL_DETAILS.iloc[:,6:7].style.set_properties(**{'text-align': 'center'}).to_excel(writer, sheet_name=f'{Destination_Schema}_COL_DETAILS', startrow=0, startcol=7, header=True, index=False)\n",
    "    combined_df_KAIZEN_COL_DETAILS.iloc[:,7:9].style.set_properties(**{'text-align': 'center'}).to_excel(writer, sheet_name=f'{Destination_Schema}_COL_DETAILS', startrow=0, startcol=8, header=True, index=False)\n",
    "    \n",
    "    # Get the xlsxwriter workbook and worksheet oBNects.\n",
    "    workbook1  = writer.book\n",
    "    worksheet1 = writer.sheets[f'{Destination_Schema}_COUNTS']\n",
    "    worksheet1.set_zoom(100)\n",
    "\n",
    "    worksheet1.set_column('A:A', 38)\n",
    "    worksheet1.set_column('B:B', 38)\n",
    "    worksheet1.set_column('C:C', 29)\n",
    "    worksheet1.set_column('D:D', 29)\n",
    "    worksheet1.set_column('E:E', 30)\n",
    "    worksheet1.set_column('F:F', 28)\n",
    "    worksheet1.set_column('G:G', 28)\n",
    "    worksheet1.set_column('H:H', 30)\n",
    "    worksheet1.set_column('I:I', 30)\n",
    "    worksheet1.set_column('J:J', 30)\n",
    "    worksheet1.set_column('K:K', 30)\n",
    "\n",
    "    border_fmt = workbook1.add_format({'bottom':2, 'top':2, 'left':2, 'right':2})\n",
    "    worksheet1.conditional_format(xlsxwriter.utility.xl_range(0, 0, len(combined_df_KAIZEN.iloc[:,0:10]) , len(combined_df_KAIZEN.iloc[:,0:10].columns)  ), {'type': 'no_errors', 'format': border_fmt})\n",
    "\n",
    "    wrap_format = workbook1.add_format({'bold': True, 'text_wrap': True, 'align':'center','border': 0})\n",
    "    worksheet1.write(0, 4, \"ROW_DIFFERENCE \\n (SQLServerDW - SQLServerSRC)\", wrap_format)\n",
    "    worksheet1.write(0, 7, \"COL_DIFFERENCE \\n (SQLServerDW - SQLServerSRC)\", wrap_format)\n",
    "\n",
    "\n",
    "    yellow_sum_col = workbook1.add_format({'bg_color': '#FCF8D0','bold': True,'text_wrap': True,'align':'center','border': 0}) # white cell font color\n",
    "    yellow_sum_col.set_font_name('Times New Roman')\n",
    "    yellow_sum_col.set_font_size(10)\n",
    "    yellow_sum_col.set_align('center')\n",
    "    for i in range(2,len(combined_df_KAIZEN.iloc[:,0:10])+2):\n",
    "        worksheet1.write(\"E{0}\".format(i), '=D{0}-C{0}'.format(i), yellow_sum_col)\n",
    "        worksheet1.write(\"H{0}\".format(i), '=G{0}-F{0}'.format(i), yellow_sum_col)\n",
    "        i=i+1\n",
    "\n",
    "\n",
    "\n",
    "    # Set the format for the red color\n",
    "    red_format = workbook1.add_format({'bg_color': 'red', 'bold': True})\n",
    "    for i in range(2,len(combined_df_KAIZEN.iloc[:,0:10])+2):\n",
    "        worksheet1.conditional_format('A{0}:A{0}'.format(i), {'type': 'formula',\n",
    "                                               'criteria': '=(D{0}-C{0})>0'.format(i),\n",
    "                                               'format': red_format})\n",
    "        worksheet1.conditional_format('B{0}:B{0}'.format(i), {'type': 'formula',\n",
    "                                               'criteria': '=(D{0}-C{0})>0'.format(i),\n",
    "                                               'format': red_format})\n",
    "        worksheet1.conditional_format('E{0}:E{0}'.format(i), {'type': 'formula',\n",
    "                                               'criteria': '=(D{0}-C{0})>0'.format(i),\n",
    "                                               'format': red_format})\n",
    "        worksheet1.conditional_format('H{0}:H{0}'.format(i), {'type': 'formula',\n",
    "                                               'criteria': '=(G{0}-F{0})>0'.format(i),\n",
    "                                               'format': red_format})\n",
    "\n",
    "    #-------------------------------------------------------------------\n",
    "    #-------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    workbook2  = writer.book\n",
    "    worksheet2 = writer.sheets[f'{Destination_Schema}_COL_DETAILS']\n",
    "    worksheet2.set_zoom(100)\n",
    "\n",
    "    worksheet2.set_column('A:A', 38)\n",
    "    worksheet2.set_column('B:B', 38)\n",
    "    worksheet2.set_column('C:C', 27)\n",
    "    worksheet2.set_column('D:D', 27)\n",
    "    worksheet2.set_column('E:E', 30)\n",
    "    worksheet2.set_column('F:F', 30)\n",
    "    worksheet2.set_column('G:G', 30)\n",
    "    worksheet2.set_column('H:H', 32)\n",
    "    worksheet2.set_column('I:I', 31)\n",
    "    worksheet2.set_column('J:J', 31)\n",
    "\n",
    "    border_fmt = workbook2.add_format({'bottom':2, 'top':2, 'left':2, 'right':2})\n",
    "    worksheet2.conditional_format(xlsxwriter.utility.xl_range(0, 0, len(combined_df_KAIZEN_COL_DETAILS.iloc[:,0:11]) , len(combined_df_KAIZEN_COL_DETAILS.iloc[:,0:11].columns)  ), {'type': 'no_errors', 'format': border_fmt})\n",
    "\n",
    "    wrap_format = workbook2.add_format({'bold': True, 'text_wrap': True, 'align':'center','border': 0})\n",
    "    worksheet2.write(0, 4, \"COLUMN_NAME_DIFFERENCE \\n (COLUMN_NAME_DW - COLUMN_NAME_SRC)\", wrap_format)\n",
    "    \n",
    "    yellow_sum_col = workbook1.add_format({'bg_color': '#FCF8D0','bold': True,'text_wrap': True,'align':'center','border': 0}) # white cell font color\n",
    "    yellow_sum_col.set_font_name('Times New Roman')\n",
    "    yellow_sum_col.set_font_size(10)\n",
    "    yellow_sum_col.set_align('center')\n",
    "    for i in range(2,len(combined_df_KAIZEN_COL_DETAILS.iloc[:,0:11])+2):\n",
    "        worksheet2.write(\"E{0}\".format(i), '=SUBSTITUTE(D{0},\"_\",\"\") =SUBSTITUTE(C{0},\"_\",\"\")'.format(i), yellow_sum_col)\n",
    "        i=i+1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    green_format = workbook2.add_format({'bg_color': '#92D050', 'bold': True})\n",
    "    yellow_format = workbook2.add_format({'bg_color': 'yellow', 'bold': True})\n",
    "    purple_format = workbook2.add_format({'bg_color': '#BB89F3', 'bold': True}) \n",
    "    red_format = workbook2.add_format({'bg_color': '#DD3F3F', 'bold': True})\n",
    "    \n",
    "    for i in range(2,len(combined_df_KAIZEN_COL_DETAILS.iloc[:,0:10])+2):\n",
    "        worksheet2.conditional_format('H{0}:H{0}'.format(i), {'type': 'formula',\n",
    "                                                   'criteria': '=H{0}=0'.format(i),\n",
    "                                                   'format': green_format})\n",
    "        worksheet2.conditional_format('H{0}:H{0}'.format(i), {'type': 'formula',\n",
    "                                                   'criteria': '=H{0}=1'.format(i),\n",
    "                                                   'format': yellow_format})\n",
    "        worksheet2.conditional_format('H{0}:H{0}'.format(i), {'type': 'formula',\n",
    "                                                   'criteria': '=H{0}=2'.format(i),\n",
    "                                                   'format': purple_format})\n",
    "        worksheet2.conditional_format('H{0}:H{0}'.format(i), {'type': 'formula',\n",
    "                                                   'criteria': '=H{0}=3'.format(i),\n",
    "                                                   'format': red_format})\n",
    "    writeCoverSheet(writer)\n",
    "    writer.close()\n",
    "    writer.handles = None\n",
    "except PermissionError:\n",
    "    notification.notify(\n",
    "    title = f\"PERMISSION ERROR\",\n",
    "    message = \"Your file is EITHER Open or Being currently used. Kindly close it and rerun the code\",\n",
    "    timeout = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4246c95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b6caf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken to load the file:\n",
      "-----------------------------------\n",
      "00:01:23.05\n"
     ]
    }
   ],
   "source": [
    "end_fl_time = time.time()\n",
    "hours, rem = divmod(end_fl_time-st_fl_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "\n",
    "print(\"Total time taken to load the file:\")\n",
    "print(\"-----------------------------------\")\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "\n",
    "notification.notify(\n",
    "title = f\"The {Destination_Schema} COUNT file\",\n",
    "message = \"The Excel file is now generated. TIME TAKEN {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds),\n",
    "timeout = 15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b82215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
